{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97d898af",
   "metadata": {},
   "source": [
    "# Les Réseaux de Neurones Artificiels (RNA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4d3145",
   "metadata": {},
   "source": [
    "## Généralités"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ace494f",
   "metadata": {},
   "source": [
    "Le réseau de neurones artificiel est l'un des alogorithmes de l'IA les plus sophistiqués. A la base inspiré des neuronnes biologiques, il est capable d'apprendre à réaliser n'importe quelle tâche (conduire une voiture, reconnaître et classer des photos, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14cb642",
   "metadata": {},
   "source": [
    " ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d668c334",
   "metadata": {},
   "source": [
    "Le machine learning est un domaine de l'intelligence articiel qui consiste à programmer une machine pour que celle-ci apprenne à réaliser des tâches en étudiant des exemples (données) de ses derniers.\n",
    "\n",
    "Quelques modèles de machine learning et leurs algorithmes:\n",
    "\n",
    "1. Les modèles linéaires: algorithmes de descente de gradient\n",
    "\n",
    "2. Les arbres de décision : algorithme CART\n",
    "\n",
    "3. Les support vector machine : algorithme de marge maximum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b63b4d2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9874f1d1",
   "metadata": {},
   "source": [
    "Le deep learning est un domaine du machine learning et sous-domaine de l'IA où au lieu de d'entrainer la machine avec des modèles, on developpe des réseaux de neurones articiels. Un RNA est un ensemble de fonctions interconnéctées.\n",
    "\n",
    "Plus le RNA est profond (contient plusieurs fonctions), plus la machine apprend à réaliser des tâches complexes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b224a755",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b890b3f8",
   "metadata": {},
   "source": [
    "En biologie, les neurones sont des cellules excitables connectées les unes aux autre ayant pour rôles de transmettre des informations dans notre système nerveux. Chaque neuronnes comprend:\n",
    "\n",
    "1. Des dendrites terminées par des synapses qui sont des portes d'entrée de l'information ou signal en provenance du neurone précédent. Le signal entrant peut être excitateur (+1) ou inhibiteur (-1). Lorsque l'accumulation de ces signaux dépassent un certain seuil, le neurone s'active et produit un signal électrique.\n",
    "\n",
    "2. Un corps cellulaire: il sert de canal de circulation au signal électrique produit par le neurone suite à son activation.\n",
    "\n",
    "3. Un axone qui est un ensemble de terminaisons par lesquelles le signal électrique est envoyé au neurone suivant. Ce dernier réagira de la même manière que ces prédécesseurs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72713168",
   "metadata": {},
   "source": [
    "### Historique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e87082",
   "metadata": {},
   "source": [
    "1943: Invention des premiers neurones artificiels par Warren S. McCulloch et Walter Pitts\n",
    "\n",
    "En machine learning, la modélisation de ces réseaux de neurones donne ce que l'on appelle réseaux de neurones articiels.\n",
    "Un RNA comprend :\n",
    "\n",
    "Des fonctions de transfert dont chacune correspond à un neuronne. Une fonction de transfert f prend en entrée des signaux x1, x2, ..., xn et fournit en sortie, un signal y."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218f9590",
   "metadata": {},
   "source": [
    "La fonction de transfert y <- f(x1, ..., xn) renferme deux étapes: \n",
    "\n",
    "1. l'agrégation : somme pondérée des entrées de f. Les coefficients de pondération sont W1, ..., Wn. Chaque coef représente en fait, l'activité synaptique, c'est à dire la nature du signal (excitateur ou inhibiteur).\n",
    "\n",
    "2. L'activation: y = 1 si f >=0, et y = 0 sinon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb32c61",
   "metadata": {},
   "source": [
    "Faiblesses des premiers RNA:\n",
    "\n",
    "1. Absence d'apprentissage automatique à partir de données.\n",
    "\n",
    "2. Difficulté du choix des pondérations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3f5a34",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abe467a",
   "metadata": {},
   "source": [
    "1950 : Computing machinery and intelligence, by Alan Turing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315f81c4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a71b6a",
   "metadata": {},
   "source": [
    "1957 : Invention du perceptron par Frank Rosenblatt\n",
    "\n",
    "Il s'agit d'une amélioration du premier neurone artificiel. Il a l'avantage de comporter:\n",
    "Un algorithme d'apprentissage lui permettant de déterminer les différentes pondérations W1, ..., Wn.\n",
    "\n",
    "Il est inspiré de la théorie de Donald Hebb qui stipule que : Lorsque 2 neurones sont excités conjointement, ils renforcent leurs liens synaptiques, c'est-à-dire leurs connections. En neuroscience, cela s'appelle la plasticité synaptique. C'est cette plasticité synaptique modélisée qui permet à la machine:\n",
    "\n",
    "1. de construire sa mémoire;\n",
    "2. d'apprendre de nouvelles choses;\n",
    "3. d'effectuer des associations\n",
    "\n",
    "Le perceptron permet donc d'entraîner un neurone artificiel sur des données de références (X, y) pour que celui-ci renforce ses paramètres W, à chaque fois qu'une entrée X est activée en même temps qu'une sortie y présente dans ces données.\n",
    "\n",
    "La mise à jour des W est donnée par:\n",
    "\n",
    "W = W + alpha * (y_true - y)*X\n",
    "\n",
    "Où: y_true : sortie de référence\n",
    "y : sortie produite apr le neurone\n",
    "X : entrée du neurone\n",
    "alpha : vitesse d'apprentissage\n",
    "\n",
    "A chaque fois que les W sont renforcés, la fonction de transfert se trouvera renforcée également... Ce qui accélère l'atteinte du seuil d'activation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d17475",
   "metadata": {},
   "source": [
    "Faiblesse du perceptron: il s'agit d'un modèle linéaire. Il ne peut donc pas résoudre certains problèmes (les problèmes non linéaires)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526775a4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53a4dd9",
   "metadata": {},
   "source": [
    "1986 : Invention du perceptron multicouche par Geoffrey Hinton.\n",
    "\n",
    "Il s'agit du premier véritable réseau de neurones artificiels.\n",
    "\n",
    "Le perceptron est un modèle linéaire, par ex : f = W1*X1 + W2*X2 + b, b étant le biais. Cette droite peut servir à séparer deux classes. Un seul perceptron ne saurait suffir pour la modélisation des phénomènes réels. On choisit donc de connecter ensemble plusieurs perceptrons. Example :\n",
    "\n",
    "1. f1 = W11*X1 + W12*X2 + b1 avec y1 = 1 si f1 >=0 et y1 = 0 sinon\n",
    "\n",
    "2. f2 = W21*X1 + W22*X2 + b2 avec y2 = 1 si f2 >=0 et y2 = 0 sinon\n",
    "\n",
    "3. f3 = W31*y1 + W32*y2 + b3 avec y3 = 1 si f1 >=0 et y3 = 0 sinon\n",
    "\n",
    "La représentation de la sortie finale en fonction des entrées n'est pas linéaire.\n",
    "Nous venons de construire un perceptron multicouche constitué d'une première couche de 2 neurones et d'une dernière couche de 1 neurone.\n",
    "\n",
    "On peut construire un réseau de neuronne avec autant de couche qu'on souhaite. \n",
    "Cependant une question se pose: Comment déterminer tous les coefficients du réseau de sorte à obtenir un bon réseau?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bc5218",
   "metadata": {},
   "source": [
    "La détermination des coefficients du réseau de neurones se fait grâce à la technique de back-propagation. La back-propagation consiste à déterminer comment les sorties varient en fonction des paramètres (W, b) présents dans chaque couche. Pour ce faire, on calcule une chaine de gradients indiquant comment la sortie finale varie en fonction de la dernière couche, comment la dernière couche varie en fonction de l'avant dernière, et ainsi de suite jusqu'à comment la deuxième couche varie en fonction de la première. \n",
    "\n",
    "Exemple : ∂y_final/∂f4, ∂f4/∂f3, ∂f3/∂f2, ∂f2/∂f1 \n",
    "\n",
    "où les fonctions f correspondent chacune à une couche et peuvent êtres à la valeurs dans Rn (ces valeurs constituent les fonctions de transfert de chaque couche). \n",
    "\n",
    "Grâce aux gradients on peut alors mettre à jour les paramètres (W, b) de chaque couche de telle sorte à ce qu'il minimsent l'erreur entre la sortie du modèle et la réponse espérée y_true. Pour cela, on utilise l'algorithme de descente de gradient:\n",
    "\n",
    "W = W + alpha * ∂Erreur / ∂W"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06de8b86",
   "metadata": {},
   "source": [
    "### Résumé"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe184dc",
   "metadata": {},
   "source": [
    "Développer un réseau de neurones passe par 4 étapes:\n",
    "\n",
    "1. Le forward propagation qui consiste à faire circuler les donneées de l'entrée du réseau jusqu'à sa sortie.\n",
    "\n",
    "2. Cost function (fonction coût) qui évalue l'erreur entre la sortie y et celle que l'on désire avoir, y_true.\n",
    "\n",
    "3. Backward propagation qui mesure la variation de la fonction coût en fonction des couches du modèles depuis la dernière à la première.\n",
    "\n",
    "4. Gradient Descent pour Correction ou mise à jour de chaque paramètres W du modèle. \n",
    "\n",
    "Puis la boucle reprend..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3a4cd2",
   "metadata": {},
   "source": [
    "### Le deep learning moderne"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820b2bf1",
   "metadata": {},
   "source": [
    "Le modèle de perceptron multicouche a continué son évolution jusqu'à l'apparition de nouvelles fonctions d'activation telles les fonctions logistique, tanh, ReLU (rectfied Linear Unit)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30602984",
   "metadata": {},
   "source": [
    "1990 : Invention des premières variantes des perceptrons multicouches, réseaux de neurones convolutifs par Yann LeCun.\n",
    "\n",
    "Cette invention a permis de faire des reconnaissances d'images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ef7d47",
   "metadata": {},
   "source": [
    "1997 : Invention d'une autre variance du perceptron multicouche appelée Réseaux de neurones récurrents LSTMs très pratique pour les séries temporelles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962e2ec8",
   "metadata": {},
   "source": [
    "Pour être entraîné, un réseau de neurone necessite une très grande quantité de données. Aussi un tel entrainement a besoin d'ordinateur ultra-puissant pour effectuer les calculs complexes. Toutes raisons expliquent le retard du développement du deep learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382ee46d",
   "metadata": {},
   "source": [
    "2012 : Cette date correspond à la date où le deep learning a pris son envol suite à un concours dénommé ImageNet remporte par Geoffrey Hinton et son équipe."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
